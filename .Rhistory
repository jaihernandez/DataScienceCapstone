data <- gsub("\\b(\\w+)\\s+\\1\\b","\\1",data) # remove duplicate words in sequence (eg.
data <- replace_contraction(data)
data <- removePunctuation(data) # remove all other punctuation
data <- str_replace_all(data, "[[:punct:]]", " ")
data <- stripWhitespace(data) # remove excess white space
data <- gsub("^[[:space:]]","",data) # make sure lines doesn't begin with space
data <- gsub("[[:space:]]$","",data) # make sure lines doesn't end with space
data <- removeWords(data, "END" )
}
else {
f <- file(paste0(data_dir,"/profanity.txt"))
profanities<-readLines(f,encoding="UTF-8")
close(f)
# profanitiesDatafile <- Corpus(VectorSource(profanities))
#
# data <- tm_map(data, content_transformer(removeWords, profanitiesDatafile))
data <- tolower(data) # convert to lowercase
data <- removeWords(data, c(profanities, "$"))
data <- removeWords(data, stopwords("english"))
data <- removeNumbers(data) # remove numbers
data <- replace_contraction(data)
data <- removePunctuation(data, ucp = TRUE) # remove all other punctuation
data <- stripWhitespace(data) # remove excess white space
data <- gsub("^[[:space:]]","",data) # make sure lines doesn't begin with space
data <- gsub("[[:space:]]$","",data) # make sure lines doesn't end with space
}
}
sample_clean <- cleanData(sample, TRUE)
c_sample_clean <- VCorpus(VectorSource(sample_clean))
TwogramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
twogramMatrix <- TermDocumentMatrix(c_sample_clean, control=list(tokenize = TwogramTokenizer))
twoFreq <- findFreqTerms(twogramMatrix,lowfreq=10)
twogramRowSum <- rowSums(as.matrix(twogramMatrix[twoFreq,]))
# memory.limit(size=60000)
# twogramRowSum <- rowSums(as.matrix(twogramMatrix))
barplot(twogramRowSum[1:20], horiz=F, cex.names=0.8, xlab="twograms",
ylab="Frequency",las=2,names.arg=names(twogramRowSum[1:20]),
main="Top 20 twogram with the highest frequency")
wordcloud(names(twogramRowSum), twogramRowSum, max.words = 100, colors = brewer.pal(6, "Dark2"))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigramMatrix <- TermDocumentMatrix(c_sample_clean, control=list(tokenize = TrigramTokenizer))
triFreq<-findFreqTerms(trigramMatrix,lowfreq=10)
trigramRowSum<-rowSums(as.matrix(trigramMatrix[triFreq,]))
barplot(trigramRowSum[1:20], horiz=F, cex.names=0.8,
ylab="Frequency",las=2,names.arg=names(trigramRowSum[1:20]),
main="Top 20 trigram with the highest frequency")
data_dir <- c("~/Work/Training/Data Science Coursera/10-Capstone/Final_Project")
cleanData <- function(data, replace_all) {
library(tm)
# Profanity filtering
f <- file(paste0(data_dir,"/profanity.txt"))
profanities<-readLines(f,encoding="UTF-8")
close(f)
# profanitiesDatafile <- Corpus(VectorSource(profanities))
#
# data <- tm_map(data, content_transformer(removeWords, profanitiesDatafile))
data <- tolower(data) # convert to lowercase
data <- removeWords(data, profanities)
data <- removeWords(data, stopwords("english"))
data <- removeNumbers(data) # remove numbers
data <- replace_contraction(data)
pontuacao <-  '[.,!:;?]|:-\\)|:-\\(|:\\)|:\\(|:D|=D|8\\)|:\\*|=\\*|:x|:X|:o|:O|:~\\(|T\\.T|Y\\.Y|S2|<3|:B|=B|=3|:3'
data <- gsub(pontuacao," END ",data) # substitute selected ponctuation (including smileys) with the word END
data <- gsub("$"," END",data) # make sure every line ends with an END
data <- gsub("\\b(\\w+)\\s+\\1\\b","\\1",data) # remove duplicate words in sequence (eg.                     data <- removePunctuation(data) # remove all other punctuation
data <- str_replace_all(data, "[[:punct:]]", "")
data <- stripWhitespace(data) # remove excess white space
data <- gsub("^[[:space:]]","",data) # make sure lines doesn't begin with space
data <- gsub("[[:space:]]$","",data) # make sure lines doesn't end with space
data <- removeWords(data, "END" )
}
sample_clean <- cleanData(sample)
c_sample_clean <- VCorpus(VectorSource(sample_clean))
TwogramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
twogramMatrix <- TermDocumentMatrix(c_sample_clean, control=list(tokenize = TwogramTokenizer))
twoFreq <- findFreqTerms(twogramMatrix,lowfreq=10)
twogramRowSum <- rowSums(as.matrix(twogramMatrix[twoFreq,]))
# memory.limit(size=60000)
# twogramRowSum <- rowSums(as.matrix(twogramMatrix))
barplot(twogramRowSum[1:20], horiz=F, cex.names=0.8, xlab="twograms",
ylab="Frequency",las=2,names.arg=names(twogramRowSum[1:20]),
main="Top 20 twogram with the highest frequency")
wordcloud(names(twogramRowSum), twogramRowSum, max.words = 100, colors = brewer.pal(6, "Dark2"))
data_dir <- c("~/Work/Training/Data Science Coursera/10-Capstone/Final_Project")
cleanData <- function(data) {
library(tm)
# Profanity filtering
f <- file(paste0(data_dir,"/profanity.txt"))
profanities<-readLines(f,encoding="UTF-8")
close(f)
# profanitiesDatafile <- Corpus(VectorSource(profanities))
#
# data <- tm_map(data, content_transformer(removeWords, profanitiesDatafile))
data <- tolower(data) # convert to lowercase
data <- removeWords(data, profanities)
data <- removeWords(data, stopwords("english"))
data <- removeNumbers(data) # remove numbers
data <- replace_contraction(data)
pontuacao <-  '[.,!:;?]|:-\\)|:-\\(|:\\)|:\\(|:D|=D|8\\)|:\\*|=\\*|:x|:X|:o|:O|:~\\(|T\\.T|Y\\.Y|S2|<3|:B|=B|=3|:3'
data <- gsub(pontuacao," END ",data) # substitute selected ponctuation (including smileys) with the word END
data <- gsub("$"," END",data) # make sure every line ends with an END
data <- gsub("\\b(\\w+)\\s+\\1\\b","\\1",data) # remove duplicate words in sequence (eg.                     data <- removePunctuation(data) # remove all other punctuation
data <- str_replace_all(data, "[[:punct:]]", "")
data <- stripWhitespace(data) # remove excess white space
data <- gsub("^[[:space:]]","",data) # make sure lines doesn't begin with space
data <- gsub("[[:space:]]$","",data) # make sure lines doesn't end with space
data <- removeWords(data, "END" )
}
sample_clean <- cleanData(sample)
c_sample_clean <- VCorpus(VectorSource(sample_clean))
row = grep(sample_clean, "$" )
row
TwogramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
twogramMatrix <- TermDocumentMatrix(c_sample_clean, control=list(tokenize = TwogramTokenizer))
twoFreq <- findFreqTerms(twogramMatrix,lowfreq=10)
twogramRowSum <- rowSums(as.matrix(twogramMatrix[twoFreq,]))
# memory.limit(size=60000)
# twogramRowSum <- rowSums(as.matrix(twogramMatrix))
barplot(twogramRowSum[1:20], horiz=F, cex.names=0.8, xlab="twograms",
ylab="Frequency",las=2,names.arg=names(twogramRowSum[1:20]),
main="Top 20 twogram with the highest frequency")
wordcloud(names(twogramRowSum), twogramRowSum, max.words = 100, colors = brewer.pal(6, "Dark2"))
grep(sample_clean, "$ $")
grep(sample_clean, "$$")
grep(sample_clean, "country")
data_dir <- c("~/Work/Training/Data Science Coursera/10-Capstone/Final_Project")
cleanData <- function(data) {
library(tm)
# Profanity filtering
f <- file(paste0(data_dir,"/profanity.txt"))
profanities<-readLines(f,encoding="UTF-8")
close(f)
# profanitiesDatafile <- Corpus(VectorSource(profanities))
#
# data <- tm_map(data, content_transformer(removeWords, profanitiesDatafile))
data <- tolower(data) # convert to lowercase
data <- removeWords(data, profanities)
data <- removeWords(data, stopwords("english"))
data <- removeNumbers(data) # remove numbers
data <- replace_contraction(data)
pontuacao <-  '[.,!:;?]|:-\\)|:-\\(|:\\)|:\\(|:D|=D|8\\)|:\\*|=\\*|:x|:X|:o|:O|:~\\(|T\\.T|Y\\.Y|S2|<3|:B|=B|=3|:3'
data <- gsub(pontuacao," END ",data) # substitute selected ponctuation (including smileys) with the word END
data <- gsub("$"," END",data) # make sure every line ends with an END
data <- gsub("\\b(\\w+)\\s+\\1\\b","\\1",data) # remove duplicate words in sequence (eg.
data <- removePunctuation(data) # remove all other punctuation
data <- str_replace_all(data, "[[:punct:]]", "")
data <- stripWhitespace(data) # remove excess white space
data <- gsub("^[[:space:]]","",data) # make sure lines doesn't begin with space
data <- gsub("[[:space:]]$","",data) # make sure lines doesn't end with space
data <- removeWords(data, "END" )
}
sample_clean <- cleanData(sample)
c_sample_clean <- VCorpus(VectorSource(sample_clean))
TwogramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
twogramMatrix <- TermDocumentMatrix(c_sample_clean, control=list(tokenize = TwogramTokenizer))
twoFreq <- findFreqTerms(twogramMatrix,lowfreq=10)
twogramRowSum <- rowSums(as.matrix(twogramMatrix[twoFreq,]))
# memory.limit(size=60000)
# twogramRowSum <- rowSums(as.matrix(twogramMatrix))
barplot(twogramRowSum[1:20], horiz=F, cex.names=0.8, xlab="twograms",
ylab="Frequency",las=2,names.arg=names(twogramRowSum[1:20]),
main="Top 20 twogram with the highest frequency")
wordcloud(names(twogramRowSum), twogramRowSum, max.words = 100, colors = brewer.pal(6, "Dark2"))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigramMatrix <- TermDocumentMatrix(c_sample_clean, control=list(tokenize = TrigramTokenizer))
triFreq<-findFreqTerms(trigramMatrix,lowfreq=10)
trigramRowSum<-rowSums(as.matrix(trigramMatrix[triFreq,]))
barplot(trigramRowSum[1:20], horiz=F, cex.names=0.8,
ylab="Frequency",las=2,names.arg=names(trigramRowSum[1:20]),
main="Top 20 trigram with the highest frequency")
data <- sample
f <- file(paste0(data_dir,"/profanity.txt"))
profanities<-readLines(f,encoding="UTF-8")
close(f)
# profanitiesDatafile <- Corpus(VectorSource(profanities))
#
# data <- tm_map(data, content_transformer(removeWords, profanitiesDatafile))
data <- tolower(data) # convert to lowercase
data <- removeWords(data, profanities)
data <- removeWords(data, stopwords("english"))
grep(data, "along way")
data_dir <- c("~/Work/Training/Data Science Coursera/10-Capstone/Final_Project")
cleanData <- function(data) {
library(tm)
# Profanity filtering
f <- file(paste0(data_dir,"/profanity.txt"))
profanities<-readLines(f,encoding="UTF-8")
close(f)
# profanitiesDatafile <- Corpus(VectorSource(profanities))
#
# data <- tm_map(data, content_transformer(removeWords, profanitiesDatafile))
data <- tolower(data) # convert to lowercase
data <- removeWords(data, profanities)
# data <- removeWords(data, stopwords("english"))
data <- removeNumbers(data) # remove numbers
data <- replace_contraction(data)
pontuacao <-  '[.,!:;?]|:-\\)|:-\\(|:\\)|:\\(|:D|=D|8\\)|:\\*|=\\*|:x|:X|:o|:O|:~\\(|T\\.T|Y\\.Y|S2|<3|:B|=B|=3|:3'
data <- gsub(pontuacao," END ",data) # substitute selected ponctuation (including smileys) with the word END
data <- gsub("$"," END",data) # make sure every line ends with an END
data <- gsub("\\b(\\w+)\\s+\\1\\b","\\1",data) # remove duplicate words in sequence (eg.
data <- removePunctuation(data) # remove all other punctuation
data <- str_replace_all(data, "[[:punct:]]", "")
data <- stripWhitespace(data) # remove excess white space
data <- gsub("^[[:space:]]","",data) # make sure lines doesn't begin with space
data <- gsub("[[:space:]]$","",data) # make sure lines doesn't end with space
data <- removeWords(data, "END" )
}
sample_clean <- cleanData(sample)
c_sample_clean <- VCorpus(VectorSource(sample_clean))
TwogramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
twogramMatrix <- TermDocumentMatrix(c_sample_clean, control=list(tokenize = TwogramTokenizer))
twoFreq <- findFreqTerms(twogramMatrix,lowfreq=10)
twogramRowSum <- rowSums(as.matrix(twogramMatrix[twoFreq,]))
# memory.limit(size=60000)
# twogramRowSum <- rowSums(as.matrix(twogramMatrix))
barplot(twogramRowSum[1:20], horiz=F, cex.names=0.8, xlab="twograms",
ylab="Frequency",las=2,names.arg=names(twogramRowSum[1:20]),
main="Top 20 twogram with the highest frequency")
wordcloud(names(twogramRowSum), twogramRowSum, max.words = 100, colors = brewer.pal(6, "Dark2"))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigramMatrix <- TermDocumentMatrix(c_sample_clean, control=list(tokenize = TrigramTokenizer))
triFreq<-findFreqTerms(trigramMatrix,lowfreq=10)
trigramRowSum<-rowSums(as.matrix(trigramMatrix[triFreq,]))
barplot(trigramRowSum[1:20], horiz=F, cex.names=0.8,
ylab="Frequency",las=2,names.arg=names(trigramRowSum[1:20]),
main="Top 20 trigram with the highest frequency")
#STarting with one worker
options(mc.cores = 1)
OnegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
# corpusSample.pruned <- tm_map(sample, removeWords, stopwords("english"))
# memory.limit(size=60000)
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
stats_df <- data.frame(rbind(stri_stats_general(twitter),
stri_stats_general(news),
stri_stats_general(blog)),
source = c("twitter", "news", "blog")
)
setwd("~/Work/Training/Data Science Coursera/10-Capstone/Final_Project")
library(stringi)
library(stringr)
library(ggplot2)
library(RColorBrewer)
library(dplyr)
library(tm)
library(wordcloud)
library(rJava)
library(RWeka)
library(textclean)
con <- file("./final/en_US/en_US.twitter.txt", open = "rb")
twitter <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)
con <- file("./final/en_US/en_US.news.txt", open = "rb")
news <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)
con <- file("./final/en_US/en_US.blogs.txt", open = "rb")
blog <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)
stats_df <- data.frame(rbind(stri_stats_general(twitter),
stri_stats_general(news),
stri_stats_general(blog)),
source = c("twitter", "news", "blog")
)
g.word.count <- ggplot(stats_df, aes(x = stats_df$Lines, y = stats_df$Chars, fill = source)) +
geom_bar(stat = "identity") +
ggtitle("Word Count") +
xlab("Number of Lines") +
ylab("Empy Lines")
g.word.count
colnames(stats_df) <- c("Total Number of Lines", "Empty Lines", "Total Number of Characters", "Number of Empty Characters" , "Source")
stats_df
memory.limit()
memory.limit(size = 72000)
memory.limit()
getwd()
setwd("~/Work/Training/Data Science Coursera/10-Capstone/Final_Project/Project App")
knitr::opts_chunk$set(echo = TRUE)
c_sample_clean <- readRDS("train.clean.rds")
c_sample_clean <- readRDS("train.clean.rds")
getwd()
c_sample_clean <- readRDS("training.clean.rds")
library(stringi)
library(stringr)
library(ggplot2)
library(RColorBrewer)
library(dplyr)
library(tm)
library(wordcloud)
library(rJava)
library(RWeka)
library(textclean)
smp_size<- floor(0.5 * length(c_sample_clean))
smp_size
smp_size<- floor(0.1 * length(c_sample_clean))
smp_size
smp_ind <- sample(seq_len(length(c_sample_clean)), size = smp_size)
c_sample_clean <- c_sample_clean[smp_ind]
c_sample_clean <- readRDS("training.clean.rds")
smp_size<- floor(0.15 * length(c_sample_clean))
smp_ind <- sample(seq_len(length(c_sample_clean)), size = smp_size)
c_sample_clean_raw <- readRDS("training.clean.rds")
smp_size<- floor(0.15 * length(c_sample_clean))
smp_ind <- sample(seq_len(length(c_sample_clean)), size = smp_size)
c_sample_clean_raw <- readRDS("training.clean.rds")
smp_size<- floor(0.15 * length(c_sample_clean))
smp_ind <- sample(seq_len(length(c_sample_clean)), size = smp_size)
c_sample_clean <- c_sample_clean_raw[smp_ind]
rm(c_sample_clean_raw)
gc()
#STarting with one worker
options(mc.cores = 1)
OnegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
memory.limit(size = 100000)
gc()
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
str(c_sample_clean)
c_sample_clean_raw <- readRDS("training.clean.rds")
smp_size<- floor(0.15 * length(c_sample_clean))
smp_ind <- sample(seq_len(length(c_sample_clean)), size = smp_size)
sample_clean <- c_sample_clean_raw[smp_ind]
c_sample_clean <- VCorpus(VectorSource(sample_clean))
gc()
#STarting with one worker
options(mc.cores = 1)
OnegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
memory.limit(size = 100000)
gc()
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
c_sample_clean_raw <- readRDS("training.clean.rds")
smp_size<- floor(0.1 * length(c_sample_clean))
smp_ind <- sample(seq_len(length(c_sample_clean)), size = smp_size)
sample_clean <- c_sample_clean_raw[smp_ind]
c_sample_clean <- VCorpus(VectorSource(sample_clean))
smp_size<- floor(0.1 * length(c_sample_clean_raw))
smp_ind <- sample(seq_len(length(c_sample_clean)), size = smp_size)
smp_ind <- sample(seq_len(length(c_sample_clean_raw)), size = smp_size)
sample_clean <- c_sample_clean_raw[smp_ind]
c_sample_clean <- VCorpus(VectorSource(sample_clean))
rm(c_sample_clean_raw)
gc()
#STarting with one worker
options(mc.cores = 2)
OnegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
memory.limit(size = 100000)
gc()
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
library(stringi)
library(stringr)
library(ggplot2)
library(RColorBrewer)
library(dplyr)
library(tm)
library(wordcloud)
library(rJava)
library(RWeka)
library(textclean)
gc()
#STarting with one worker
options(mc.cores = 2)
OnegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
memory.limit(size = 100000)
gc()
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
library(stringi)
library(stringr)
library(ggplot2)
library(RColorBrewer)
library(dplyr)
library(tm)
library(wordcloud)
library(rJava)
library(RWeka)
smp_size<- floor(0.1 * length(c_sample_clean_raw))
c_sample_clean_raw <- readRDS("training.clean.rds")
smp_size<- floor(0.1 * length(c_sample_clean_raw))
smp_ind <- sample(seq_len(length(c_sample_clean_raw)), size = smp_size)
sample_clean <- c_sample_clean_raw[smp_ind]
c_sample_clean <- VCorpus(VectorSource(sample_clean))
str(sample_clean)
str(c_sample_clean_raw)
rm(c_sample_clean_raw)
rm(sample_clean)
gc()
#STarting with one worker
options(mc.cores = 2)
OnegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
memory.limit(size = 100000)
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
OnegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
str(c_sample_clean)
library(stringi)
library(stringr)
library(ggplot2)
library(RColorBrewer)
library(dplyr)
library(tm)
library(wordcloud)
library(rJava)
library(RWeka)
library(textclean)
c_sample_clean_raw <- readRDS("training.clean.rds")
smp_size<- floor(0.1 * length(c_sample_clean_raw))
smp_ind <- sample(seq_len(length(c_sample_clean_raw)), size = smp_size)
sample_clean <- c_sample_clean_raw[smp_ind]
c_sample_clean <- VCorpus(VectorSource(sample_clean))
gc()
#STarting with one worker
options(mc.cores = 2)
OnegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
memory.limit(size = 100000)
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
TwogramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
twogramMatrix <- TermDocumentMatrix(c_sample_clean, control=list(tokenize = TwogramTokenizer))
c_sample_clean_raw <- readRDS("training.clean.rds")
smp_size<- floor(0.1 * length(c_sample_clean_raw))
smp_ind <- sample(seq_len(length(c_sample_clean_raw)), size = smp_size)
sample_clean <- c_sample_clean_raw[smp_ind]
c_sample_clean_raw <- readRDS("training.clean.rds")
smp_size<- floor(0.1 * length(c_sample_clean_raw))
smp_ind <- sample(seq_len(length(c_sample_clean_raw)), size = smp_size)
sample_clean <- c_sample_clean_raw[smp_ind]
sample_clean[which(is.na(sample_clean))] <- "NULLVALUEENTERED"
c_sample_clean <- VCorpus(VectorSource(sample_clean))
gc()
#STarting with one worker
options(mc.cores = 2)
OnegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
memory.limit(size = 100000)
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
str(c_sample_clean_raw)
test[which(is.na(sample_clean))] <- "NULLVALUEENTERED"
test <- sample_clean
?which
why <- which(is.na(sample_clean))
why
c_sample_clean_raw <- readRDS("training.clean.rds")
smp_size<- floor(0.1 * length(c_sample_clean_raw))
smp_ind <- sample(seq_len(length(c_sample_clean_raw)), size = smp_size)
sample_clean <- c_sample_clean_raw[smp_ind]
test <- sample_clean
why <- which(is.na(sample_clean))
test <- test[why]
test
str(test)
empty_lines <- which(!is.na(sample_clean))
empty_lines
c_sample_clean_raw <- readRDS("training.clean.rds")
smp_size<- floor(0.1 * length(c_sample_clean_raw))
smp_ind <- sample(seq_len(length(c_sample_clean_raw)), size = smp_size)
sample_clean <- c_sample_clean_raw[smp_ind]
not_empty_lines <- which(!is.na(sample_clean))
sample_clean <- sample_clean[not_empty_lines]
c_sample_clean <- VCorpus(VectorSource(sample_clean))
gc()
#STarting with one worker
options(mc.cores = 2)
OnegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
memory.limit(size = 100000)
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
c_sample_clean_raw <- readRDS("training.clean.rds")
smp_size<- floor(0.5 * length(c_sample_clean_raw))
smp_ind <- sample(seq_len(length(c_sample_clean_raw)), size = smp_size)
sample_clean <- c_sample_clean_raw[smp_ind]
not_empty_lines <- which(!is.na(sample_clean))
sample_clean <- sample_clean[not_empty_lines]
c_sample_clean <- VCorpus(VectorSource(sample_clean))
library(stringi)
library(stringr)
library(ggplot2)
library(RColorBrewer)
library(dplyr)
library(tm)
library(wordcloud)
library(rJava)
library(RWeka)
library(textclean)
c_sample_clean_raw <- readRDS("training.clean.rds")
smp_size<- floor(0.05 * length(c_sample_clean_raw))
smp_ind <- sample(seq_len(length(c_sample_clean_raw)), size = smp_size)
sample_clean <- c_sample_clean_raw[smp_ind]
not_empty_lines <- which(!is.na(sample_clean))
sample_clean <- sample_clean[not_empty_lines]
c_sample_clean <- VCorpus(VectorSource(sample_clean))
gc()
#STarting with one worker
options(mc.cores = 2)
OnegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
memory.limit(size = 100000)
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
memory.limit()
memory.limit(size = 1000000)
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
c_sample_clean_raw <- readRDS("training.clean.rds")
smp_size<- floor(0.02 * length(c_sample_clean_raw))
smp_ind <- sample(seq_len(length(c_sample_clean_raw)), size = smp_size)
sample_clean <- c_sample_clean_raw[smp_ind]
not_empty_lines <- which(!is.na(sample_clean))
sample_clean <- sample_clean[not_empty_lines]
c_sample_clean <- VCorpus(VectorSource(sample_clean))
gc()
#STarting with one worker
options(mc.cores = 2)
OnegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
memory.limit(size = 1000000)
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
gc()
#STarting with one worker
options(mc.cores = 4)
OnegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
memory.limit(size = 1000000)
oneGramMatrix<-as.matrix(TermDocumentMatrix(c_sample_clean, control=list(tokenize=OnegramTokenizer)))
